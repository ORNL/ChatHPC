<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>ChatHPC - ChatHPC</title>
<meta name="description" content="ChatHPC Application is the base CLI toolchain and Python API for working with and training models for ChatHPC. Latest release v25.7.1 Documentation">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="ChatHPC">
<meta property="og:title" content="ChatHPC">
<meta property="og:url" content="https://ornl.github.io/ChatHPC/">


  <meta property="og:description" content="ChatHPC Application is the base CLI toolchain and Python API for working with and training models for ChatHPC. Latest release v25.7.1 Documentation">











  

  


<link rel="canonical" href="https://ornl.github.io/ChatHPC/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "ORNL",
      "url": "https://ornl.github.io/ChatHPC/",
      "sameAs": ["https://github.com/ORNL"]
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/ChatHPC/feed.xml" type="application/atom+xml" rel="alternate" title="ChatHPC Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/ChatHPC/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--home">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/ChatHPC/">
          ChatHPC
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/ChatHPC/">Home</a>
            </li><li class="masthead__menu-item">
              <a href="/ChatHPC/news/">News</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style="background-color: #007833; background-image: linear-gradient(rgba(0, 0, 0, 0.0), rgba(0, 0, 0, 0.0)), url('');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          ChatHPC

        
      </h1>
      
        <p class="page__lead">ChatHPC Application is the base CLI toolchain and Python API for working with and training models for ChatHPC.<br /> <small><a href="https://github.com/ORNL/ChatHPC-app/releases/tag/v25.7.1">Latest release v25.7.1</a></small><br /> <small><a href="https://chathpc-app.readthedocs.io/en/latest/">Documentation</a></small>
</p>
      
      


      
      
        <p>
        
          <a href="https://github.com/ORNL/ChatHPC-app" class="btn btn--light-outline btn--large"><i class='fas fa-download'></i> ChatHPC Application</a>
        
      
    </div>
  
  
</div>



<div id="main" role="main">
  <article class="splash" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="ChatHPC">
    <meta itemprop="description" content="ChatHPC Application is the base CLI toolchain and Python API for working with and training models for ChatHPC. Latest release v25.7.1 Documentation">
    
    

    <section class="page__content" itemprop="text">
      

<div class="feature__wrapper">

  

</div>


<!-- 

<div class="feature__wrapper">

  

</div>
 -->



<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/ChatHPC/assets/images/sc25-splash.png"
                 alt="">
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">SC'25 Artifact Repository: ChatHPC for Kokkos</h2>
          

          
            <div class="archive__item-excerpt">
              <p>This repository holds the artifacts for the ChatHPC SC’25 submission.
Contained in this repo is the ChatHPC Library and corresponding CLI application and the Kokkos training and verification datasets used to train and validate ChatHPC for Kokkos.</p>

            </div>
          

          
            <p><a href="https://github.com/ORNL/ChatHPC-ChatHPCforKokkos-SC25" class="btn btn--info">GitHub Repo</a></p>
          
        </div>
      </div>
    </div>
  

</div>


<h2 id="publications">Publications</h2>

<script>
  // Ensure copytoclipboard is available for inline onclick handlers
  if (typeof window.copytoclipboard === 'undefined') {
    window.copytoclipboard = function(preId) {
      var el = document.getElementById(preId);
      if (!el) return;
      var text = el.innerText || el.textContent || "";
      if (!text.trim()) return;
      navigator.clipboard.writeText(text).catch(function () {
        var range = document.createRange();
        range.selectNodeContents(el);
        var sel = window.getSelection();
        sel.removeAllRanges();
        sel.addRange(range);
        document.execCommand("copy");
        sel.removeAllRanges();
      });
    };
  }
</script>

<div class="pub-list"><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">ChatHPC: Building the Foundations for a Productive and Trustworthy AI-Assisted HPC Ecosystem&nbsp;<span class="pub-badge pub-badge--dark">
            SC25
          </span></h3></div><div class="pub-authors">
          Pedro Valero Lara, Aaron Young, Jeffrey S. Vetter, Zheming Jin, Swaroop Pophale, Mohammad Alaul Haque Monil, Keita Teranishi, William F. Godoy
        </div><div class="pub-venue">
          SC '25: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-1" aria-expanded="false" aria-controls="bibtex-1">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://doi.org/10.1145/3712285.3759787">DOI</a><a class="btn btn--small pub-btn" href="https://dl.acm.org/doi/pdf/10.1145/3712285.3759787">PDF</a><a class="btn btn--small pub-btn" href="/ChatHPC/assets/pdf/ChatHPC-SC25.pdf">Slides</a></div><div id="bibtex-1" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-1"><code>@inproceedings{10.1145/3712285.3759787,
  author = {Valero Lara, Pedro and Young, Aaron and Vetter, Jeffrey S. and Jin, Zheming and Pophale, Swaroop and Alaul Haque Monil, Mohammad and Teranishi, Keita and Godoy, William F.},
  title = {ChatHPC: Building the Foundations for a Productive and Trustworthy AI-Assisted HPC Ecosystem},
  year = {2025},
  isbn = {9798400714665},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3712285.3759787},
  doi = {10.1145/3712285.3759787},
  abstract = {ChatHPC democratizes large language models for the high-performance computing (HPC) community by providing the infrastructure, ecosystem, and knowledge needed to apply modern generative AI technologies to rapidly create specific capabilities for critical HPC components while using relatively modest computational resources. Our divide-and-conquer approach focuses on creating a collection of reliable, highly specialized, and optimized AI assistants for HPC based on the cost-effective and fast Code Llama fine-tuning processes and expert supervision. We target major components of the HPC software stack, including programming models, runtimes, I/O, tooling, and math libraries. Thanks to AI, ChatHPC provides a more productive HPC ecosystem by boosting important tasks related to portability, parallelization, optimization, scalability, and instrumentation, among others. With relatively small datasets (on the order of KB), the AI assistants, which are created in a few minutes by using one node with two NVIDIA H100 GPUs and the ChatHPC library, can create new capabilities with Meta’s 7-billion parameter Code Llama base model to produce high-quality software with a level of trustworthiness of up to 90\% higher than the 1.8-trillion parameter OpenAI ChatGPT-4o model for critical programming tasks in the HPC software stack.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages = {458–474},
  numpages = {17},
  keywords = {Large Language Models, Productivity, Trustworthiness, High Performance Computing.},
  location = {
  },
  series = {SC &#39;25}
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-1')">
            Copy
          </button>
        </div></div><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">ChatMPI: LLM-Driven MPI Code Generation for HPC Workloads&nbsp;<span class="pub-badge ">
            SCA/HPCAsia'26
          </span></h3></div><div class="pub-authors">
          Pedro Valero-Lara, Aaron Young, Thomas Naughton III, Christian Engelmann, Al Geist, Jeffrey S. Vetter, Keita Teranishi, William F. Godoy
        </div><div class="pub-venue">
          SCA/HPCAsia '26: Proceedings of the Supercomputing Asia and International Conference on High Performance Computing in Asia Pacific Region
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-2" aria-expanded="false" aria-controls="bibtex-2">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://doi.org/10.1145/3773656.3773659">DOI</a><a class="btn btn--small pub-btn" href="https://dl.acm.org/doi/pdf/10.1145/3773656.3773659">PDF</a><a class="btn btn--small pub-btn" href="/ChatHPC/assets/pdf/ChatMPI-SCA-HPCAsia26.pdf">Slides</a></div><div id="bibtex-2" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-2"><code>@inproceedings{10.1145/3773656.3773659,
  author = {Valero-Lara, Pedro and Young, Aaron and Naughton III, Thomas and Engelmann, Christian and Geist, Al and Vetter, Jeffrey S. and Teranishi, Keita and Godoy, William F.},
  title = {ChatMPI: LLM-Driven MPI Code Generation for HPC Workloads},
  year = {2026},
  isbn = {9798400720673},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3773656.3773659},
  doi = {10.1145/3773656.3773659},
  abstract = {The Message Passing Interface (MPI) standard plays a crucial role in enabling scientific applications for parallel computing and is an essential component in high-performance computing (HPC). However, implementing MPI code manually—especially applying a proper domain decomposition and communication pattern—is a challenging and error-prone task. We present ChatMPI, an AI assistant for MPI parallelization of sequential C codes. In our analysis, we focus on testing six essential HPC workloads, which are based on Basic Linear Algebra Subprograms levels 1, 2, and 3 as well as sparse, stencil, and iterative operations. We analyze the process of creating ChatMPI by using the ChatHPC library. This lightweight large language model (LLM)–based infrastructure enables HPC experts to efficiently create and supervise trustworthy AI capabilities for critical HPC software tasks. We study the data required for training (fine-tuning) ChatMPI to generate parallel codes that not only use MPI syntax correctly but also apply HPC techniques to reduce memory communication and maximize performance by using proper work decomposition. With a relatively small training dataset composed of a few dozen prompts and fewer than 15 minutes of fine-tuning on one node equipped with two NVIDIA H100 GPUs, ChatMPI elevates trustworthiness for MPI code generation of current LLMs (e.g., Code Llama, ChatGPT-4o and ChatGPT 5). Additionally, we evaluate the performance of the MPI codes generated by ChatMPI in comparison with the ones generated by ChatGPT-4o and ChatGPT-5. The codes generated by ChatMPI provide up to a 4 \texttimes{} boost in performance by using better problem decomposition, communication patterns, and HPC techniques (e.g., communication avoiding).},
  booktitle = {Proceedings of the Supercomputing Asia and International Conference on High Performance Computing in Asia Pacific Region},
  pages = {19–30},
  numpages = {12},
  keywords = {ChatHPC, AI, LLM, MPI, HPC},
  location = {
  },
  series = {SCA/HPCAsia &#39;26}
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-2')">
            Copy
          </button>
        </div></div><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">Leveraging AI for Productive and Trustworthy HPC Software: Challenges and Research Directions&nbsp;<span class="pub-badge ">
            ISC High Performance 2025
          </span></h3></div><div class="pub-authors">
          Keita Teranishi, Harshitha Menon, William F. Godoy, Prasanna Balaprakash, David Bau, Tal Ben-Nun, Abhinav Bhatele, Franz Franchetti, Michael Franusich, Todd Gamblin, Giorgis Georgakoudis, Tom Goldstein, Arjun Guha, Steven E. Hahn, Costin Iancu, Zheming Jin, Terry Jones, Tze-Meng Low, Het Mankad, Narasinga Rao Miniskar, Mohammad Alaul Haque Monil, Daniel Nichols, Konstantinos Parasyris, Swaroop Pophale, Pedro Valero-Lara, Jeffrey S. Vetter, Samuel Williams, Aaron Young
        </div><div class="pub-venue">
          ISC High Performance 2025 International Workshops
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-3" aria-expanded="false" aria-controls="bibtex-3">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://doi.org/10.1007/978-3-032-07612-0_47">DOI</a><a class="btn btn--small pub-btn" href="https://link.springer.com/content/pdf/10.1007/978-3-032-07612-0.pdf">PDF</a></div><div id="bibtex-3" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-3"><code>@InProceedings{10.1007/978-3-032-07612-0_47,
  author=&quot;Teranishi, Keita
  and Menon, Harshitha
  and Godoy, William F.
  and Balaprakash, Prasanna
  and Bau, David
  and Ben-Nun, Tal
  and Bhatele, Abhinav
  and Franchetti, Franz
  and Franusich, Michael
  and Gamblin, Todd
  and Georgakoudis, Giorgis
  and Goldstein, Tom
  and Guha, Arjun
  and Hahn, Steven E.
  and Iancu, Costin
  and Jin, Zheming
  and Jones, Terry
  and Low, Tze-Meng
  and Mankad, Het
  and Miniskar, Narasinga Rao
  and Monil, Mohammad Alaul Haque
  and Nichols, Daniel
  and Parasyris, Konstantinos
  and Pophale, Swaroop
  and Valero-Lara, Pedro
  and Vetter, Jeffrey S.
  and Williams, Samuel
  and Young, Aaron&quot;,
  editor=&quot;Neuwirth, Sarah
  and Paul, Arnab Kumar
  and Weinzierl, Tobias
  and Carson, Erin Claire&quot;,
  title=&quot;Leveraging AI for Productive and Trustworthy HPC Software: Challenges and Research Directions&quot;,
  booktitle=&quot;High Performance Computing&quot;,
  year=&quot;2026&quot;,
  publisher=&quot;Springer Nature Switzerland&quot;,
  address=&quot;Cham&quot;,
  pages=&quot;615--625&quot;,
  abstract=&quot;We discuss the challenges and propose research directions for using AI to revolutionize the development of high-performance computing (HPC) software. AI technologies, in particular large language models, have transformed every aspect of software development. For its part, HPC software is recognized as a highly specialized scientific field of its own. We discuss the challenges associated with leveraging state-of-the-art AI technologies to develop such a unique and niche class of software and outline our research directions in the two US Department of Energy--funded projects for advancing HPC Software via AI: Ellora and Durban.&quot;,
  isbn=&quot;978-3-032-07612-0&quot;
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-3')">
            Copy
          </button>
        </div></div><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">LLM-Driven Fortran-to-C/C++ Portability for Parallel Scientific Codes&nbsp;<span class="pub-badge ">
            eScience 2025
          </span></h3></div><div class="pub-authors">
          Pedro Valero-Lara, William F. Godoy, Jose Gonzalez, Alexis Huante, Hallyma Gauthier-Chaparro, Jhonny Gonzalez
        </div><div class="pub-venue">
          2025 IEEE International Conference on eScience
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-4" aria-expanded="false" aria-controls="bibtex-4">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://doi.org/10.1109/eScience65000.2025.00083">DOI</a><a class="btn btn--small pub-btn" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=11181523">PDF</a></div><div id="bibtex-4" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-4"><code>@INPROCEEDINGS{11181523,
  author={Valero-Lara, Pedro and Godoy, William F. and Gonzalez, Jose and Huante, Alexis and Gauthier-Chaparro, Hallyma and Gonzalez, Jhonny and Tang, Yuguo Kelly and Teranishi, Keita and Vetter, Jeffrey S.},
  booktitle={2025 IEEE International Conference on eScience (eScience)},
  title={LLM-Driven Fortran-to-C/C++ Portability for Parallel Scientific Codes},
  year={2025},
  volume={},
  number={},
  pages={385-394},
  abstract={We define the fundamental practices and criteria for evaluating and using the Meta Llama 3 and OpenAI ChatGPT 3.5 and 4o large language models (LLMs) to translate parallel scientific Fortran + OpenMP and Fortran + OpenACC codes to C/C++ codes that can leverage vendor-specific libraries (CUDA, HIP) for GPU acceleration in addition to other performance-portable programming models (e.g., Kokkos, OpenMP, OpenACC). In this study, LLMs are used to translate 11 different parallel Fortran codes with some of the most popular and widely used kernels/proxies in high-performance computing (HPC): AXPY, GEMV, GEMM, Jacobi, SpMV, and the &gt;200-line Hartree-Fock application proxy, which implements a solver for quantum many-body systems. In all, we analyze the correctness and reproducibility of more than 1,650 AI-generated parallel C/C++ codes. Additionally, we evaluate the performance of Fortran codes and AI-generated C/C++ codes on two modern HPC architectures—one AMD EPYC Rome CPU with 64 cores and one NVIDIA Ampere A100 GPU. We use multi-modal prompting and fine-tuning techniques for LLMs to produce parallel scientific C/C++ codes with high levels of correctness (more than 95% of the codes are well ported) and speedups of up to an order of magnitude versus Fortran + OpenMP and Fortran + OpenACC codes on the same system.},
  keywords={Jacobian matrices;Codes;Translation;Parallel programming;Biological system modeling;Large language models;Graphics processing units;Chatbots;Reproducibility of results;Hip;AI;Large Language Models;Parallel Programming;Fortran;C/C++;OpenMP;OpenACC;CUDA;HIP;Kokkos},
  doi={10.1109/eScience65000.2025.00083},
  ISSN={2325-3703},
  month={Sep.},
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-4')">
            Copy
          </button>
        </div></div><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">Enhancing ChatPORT with CUDA-to-SYCL Kernel Translation Capability&nbsp;<span class="pub-badge ">
            SC25-W
          </span></h3></div><div class="pub-authors">
          Zheming Jin, Swaroop Pophale, Keita Teranishi
        </div><div class="pub-venue">
          SC Workshops '25: Proceedings of the SC '25 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-5" aria-expanded="false" aria-controls="bibtex-5">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://doi.org/10.1145/3731599.3767398">DOI</a><a class="btn btn--small pub-btn" href="https://dl.acm.org/doi/pdf/10.1145/3731599.3767398">PDF</a></div><div id="bibtex-5" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-5"><code>@inproceedings{10.1145/3731599.3767398,
  author = {Jin, Zheming and Pophale, Swaroop and Teranishi, Keita},
  title = {Enhancing ChatPORT with CUDA-to-SYCL Kernel Translation Capability},
  year = {2025},
  isbn = {9798400718717},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3731599.3767398},
  doi = {10.1145/3731599.3767398},
  abstract = {Large Language Models (LLMs) have shown strong capabilities in general code translation. However, code translation involving parallel programming models remains largely unexplored. This work enhances the capabilities of code LLMs in CUDA-to-SYCL kernel translation with parameter-efficient fine-tuning. The resultant fine-tuned LLM, called ChatPORT, is an effort to provide high-fidelity translations from one programming model to another. We describe the preparation of datasets from heterogeneous computing benchmarks for model fine-tuning and testing, the parameter-efficient fine-tuning of 19 open-source code models ranging in size from 0.5 to 34 billion parameters and evaluate the correctness rates of the SYCL kernels by the fine-tuned models. The experimental results show that most code models fail to translate CUDA codes to SYCL correctly. However, fine-tuning these models using a small set of CUDA and SYCL kernels can enhance the capabilities of these models in kernel translation. Depending on the sizes of the models, the correctness rate ranges from 19.9\% to 81.7\% for a test dataset of 62 CUDA kernels.},
  booktitle = {Proceedings of the SC &#39;25 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages = {524–533},
  numpages = {10},
  keywords = {CUDA, Code Translation, Generative Artificial Intelligence, Large Language Models, SYCL, Software Development},
  location = {
  },
  series = {SC Workshops &#39;25}
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-5')">
            Copy
          </button>
        </div></div><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">Large language model evaluation for high-performance computing software development&nbsp;<span class="pub-badge ">
            Special Issue
          </span></h3></div><div class="pub-authors">
          William F. Godoy, Pedro Valero-Lara, Keita Teranishi, Prasanna Balaprakash, Jeffrey S. Vetter
        </div><div class="pub-venue">
          Concurrency and Computation Practics and Experience
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-6" aria-expanded="false" aria-controls="bibtex-6">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://doi.org/10.1002/cpe.8269">DOI</a><a class="btn btn--small pub-btn" href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/cpe.8269">PDF</a></div><div id="bibtex-6" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-6"><code>@article{https://doi.org/10.1002/cpe.8269,
  author = {Godoy, William F. and Valero-Lara, Pedro and Teranishi, Keita and Balaprakash, Prasanna and Vetter, Jeffrey S.},
  title = {Large language model evaluation for high-performance computing software development},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {36},
  number = {26},
  pages = {e8269},
  keywords = {auto-parallelization, code generation, GPT, high-performance computing, large language model, programming models},
  doi = {https://doi.org/10.1002/cpe.8269},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.8269},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.8269},
  abstract = {Abstract We apply AI-assisted large language model (LLM) capabilities of GPT-3 targeting high-performance computing (HPC) kernels for (i) code generation, and (ii) auto-parallelization of serial code in C ++, Fortran, Python and Julia. Our scope includes the following fundamental numerical kernels: AXPY, GEMV, GEMM, SpMV, Jacobi Stencil, and CG, and language/programming models: (1) C++ (e.g., OpenMP [including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g., OpenMP [including offload] and OpenACC), (3) Python (e.g., numpy, Numba, cuPy, and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and KernelAbstractions.jl). Kernel implementations are generated using GitHub Copilot capabilities powered by the GPT-based OpenAI Codex available in Visual Studio Code given simple &lt;kernel&gt; + &lt;programming model&gt; + &lt;optional hints&gt; prompt variants. To quantify and compare the generated results, we propose a proficiency metric around the initial 10 suggestions given for each prompt. For auto-parallelization, we use ChatGPT interactively giving simple prompts as in a dialogue with another human including simple “prompt engineering” follow ups. Results suggest that correct outputs for C++ correlate with the adoption and maturity of programming models. For example, OpenMP and CUDA score really high, whereas HIP is still lacking. We found that prompts from either a targeted language such as Fortran or the more general-purpose Python can benefit from adding language keywords, while Julia prompts perform acceptably well for its Threads and CUDA.jl programming models. We expect to provide an initial quantifiable point of reference for code generation in each programming model using a state-of-the-art LLM. Overall, understanding the convergence of LLMs, AI, and HPC is crucial due to its rapidly evolving nature and how it is redefining human-computer interactions.},
  year = {2024}
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-6')">
            Copy
          </button>
        </div></div><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">ChatBLAS: The First AI-Generated and Portable BLAS Library&nbsp;<span class="pub-badge ">
            SC24-W
          </span></h3></div><div class="pub-authors">
          Pedro Valero-Lara, William F. Godoy, Keita Teranishi, Prasanna Balaprakash, Jeffrey S. Vetter
        </div><div class="pub-venue">
          SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-7" aria-expanded="false" aria-controls="bibtex-7">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://doi.org/10.1109/SCW63240.2024.00010">DOI</a><a class="btn btn--small pub-btn" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10820659">PDF</a></div><div id="bibtex-7" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-7"><code>@INPROCEEDINGS{10820659,
  author={Valero-Lara, Pedro and Godoy, William F. and Teranishi, Keita and Balaprakash, Prasanna and Vetter, Jeffrey S.},
  booktitle={SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  title={ChatBLAS: The First AI-Generated and Portable BLAS Library},
  year={2024},
  volume={},
  number={},
  pages={19-24},
  abstract={We present ChatBLAS, the first AI-generated and portable Basic Linear Algebra Subprograms (BLAS) library on different CPU/GPU configurations. The purpose of this study is (i) to evaluate the capabilities of current large language models (LLMs) to generate a portable and HPC library for BLAS operations and (ii) to define the fundamental practices and criteria to interact with LLMs for HPC targets to elevate the trustworthiness and performance levels of the AI-generated HPC codes. The generated C/C++ codes must be highly optimized using device-specific solutions to reach high levels of performance. Additionally, these codes are very algorithm-dependent, thereby adding an extra dimension of complexity to this study. We used OpenAI’s LLM ChatGPT and focused on vector-vector BLAS level-1 operations. ChatBLAS can generate functional and correct codes, achieving high-trustworthiness levels, and can compete or even provide better performance against vendor libraries.},
  keywords={Performance evaluation;Codes;Large language models;High performance computing;Linear algebra;Programming;Libraries;System-on-chip;Prompt engineering;Hip;Julia;JACC;metaprogramming;performance portability;high-bandwidth on-chip memory},
  doi={10.1109/SCW63240.2024.00010},
  ISSN={},
  month={Nov},
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-7')">
            Copy
          </button>
        </div></div><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">Comparing Llama-2 and GPT-3 LLMs for HPC Kernels Generation&nbsp;<span class="pub-badge ">
            LCPC'23
          </span></h3></div><div class="pub-authors">
          Pedro Valero-Lara, Alexis Huante, Mustafa Al Lail, William F. Godoy, Keita Teranishi, Prasanna Balaprakash, Jeffrey S. Vetter
        </div><div class="pub-venue">
          Languages and Compilers for Parallel Computing
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-8" aria-expanded="false" aria-controls="bibtex-8">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://doi.org/10.1007/978-3-032-02436-7_2">DOI</a><a class="btn btn--small pub-btn" href="https://link.springer.com/content/pdf/10.1007/978-3-032-02436-7.pdf">PDF</a></div><div id="bibtex-8" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-8"><code>@InProceedings{10.1007/978-3-032-02436-7_2,
  author=&quot;Valero-Lara, Pedro
  and Huante, Alexis
  and Al Lail, Mustafa
  and Godoy, William F.
  and Teranishi, Keita
  and Balaprakash, Prasanna
  and Vetter, Jeffrey S.&quot;,
  editor=&quot;Dietz, Henry&quot;,
  title=&quot;Comparing Llama-2 and GPT-3 LLMs for HPC Kernels Generation&quot;,
  booktitle=&quot;Languages and Compilers for Parallel Computing&quot;,
  year=&quot;2026&quot;,
  publisher=&quot;Springer Nature Switzerland&quot;,
  address=&quot;Cham&quot;,
  pages=&quot;20--32&quot;,
  abstract=&quot;We evaluate the use of the open-source Llama-2 model for generating well-known, high-performance computing kernels (e.g., AXPY, GEMV, GEMM) on different parallel programming models and languages (e.g., C++: OpenMP, OpenMP Offload, OpenACC, CUDA, HIP; Fortran: OpenMP, OpenMP Offload, OpenACC; Python: numpy, Numba, pyCUDA, cuPy; and Julia: Threads, CUDA.jl, AMDGPU.jl). We built upon our previous work that is based on the OpenAI Codex, which is a descendant of GPT-3, to generate similar kernels with simple prompts via GitHub Copilot. Our goal is to compare the accuracy of Llama-2 and our original GPT-3 baseline by using a similar metric. Llama-2 has a simplified model that shows competitive or even superior accuracy. We also report on the differences between these foundational large language models as generative AI continues to redefine human-computer interactions. Overall, Copilot generates codes that are more reliable but less optimized, whereas codes generated by Llama-2 are less reliable but more optimized when correct.&quot;,
  isbn=&quot;978-3-032-02436-7&quot;
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-8')">
            Copy
          </button>
        </div></div><div class="pub-card">
      <div class="pub-card__header">
        <h3 class="pub-title">Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation&nbsp;<span class="pub-badge ">
            ICPP Workshops'23
          </span></h3></div><div class="pub-authors">
          William Godoy, Pedro Valero-Lara, Keita Teranishi, Prasanna Balaprakash, Jeffrey Vetter
        </div><div class="pub-venue">
          ICPP Workshops '23: Proceedings of the 52nd International Conference on Parallel Processing Workshops
        </div><div class="pub-actions"><button class="btn btn--primary btn--small pub-btn" type="button" data-toggle="mm-collapse" data-target="bibtex-9" aria-expanded="false" aria-controls="bibtex-9">
            Cite This
          </button><a class="btn btn--small pub-btn" href="https://dl.acm.org/doi/10.1145/3605731.3605886">DOI</a><a class="btn btn--small pub-btn" href="https://dl.acm.org/doi/pdf/10.1145/3605731.3605886">PDF</a></div><div id="bibtex-9" class="pub-bibtex mm-collapse" hidden="">
          <pre id="bibtex-code-9"><code>@inproceedings{10.1145/3605731.3605886,
  author = {Godoy, William and Valero-Lara, Pedro and Teranishi, Keita and Balaprakash, Prasanna and Vetter, Jeffrey},
  title = {Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation},
  year = {2023},
  isbn = {9798400708428},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3605731.3605886},
  doi = {10.1145/3605731.3605886},
  abstract = {We evaluate AI-assisted generative capabilities on fundamental numerical kernels in high-performance computing (HPC), including AXPY, GEMV, GEMM, SpMV, Jacobi Stencil, and CG. We test the generated kernel codes for a variety of language-supported programming models, including (1) C++ (e.g., OpenMP [including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g., OpenMP [including offload] and OpenACC), (3) Python (e.g., numpy, Numba, cuPy, and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and KernelAbstractions.jl). We use the GitHub Copilot capabilities powered by the GPT-based OpenAI Codex available in Visual Studio Code as of April 2023 to generate a vast amount of implementations given simple &lt;kernel&gt; + &lt;programming model&gt; + &lt;optional hints&gt; prompt variants. To quantify and compare the results, we propose a proficiency metric around the initial 10 suggestions given for each prompt. Results suggest that the OpenAI Codex outputs for C++ correlate with the adoption and maturity of programming models. For example, OpenMP and CUDA score really high, whereas HIP is still lacking. We found that prompts from either a targeted language such as Fortran or the more general-purpose Python can benefit from adding code keywords, while Julia prompts perform acceptably well for its mature programming models (e.g., Threads and CUDA.jl). We expect for these benchmarks to provide a point of reference for each programming model’s community. Overall, understanding the convergence of large language models, AI, and HPC is crucial due to its rapidly evolving nature and how it is redefining human-computer interactions.},
  booktitle = {Proceedings of the 52nd International Conference on Parallel Processing Workshops},
  pages = {136–144},
  numpages = {9},
  keywords = {GPT, GitHub Copilot, HPC, LLM, OpenAI Codex, generative AI, high-performance computing, large language models, numerical kernels, programming models},
  location = {Salt Lake City, UT, USA},
  series = {ICPP Workshops &#39;23}
}
</code></pre>
          <button class="btn btn--small pub-btn" type="button" onclick="copytoclipboard('bibtex-code-9')">
            Copy
          </button>
        </div></div></div>

<section id="team" class="mm-team">
  <h2>Team</h2>

  <div class="mm-team__grid"><div class="mm-team__member">
        <a class="mm-team__link" href="https://github.com/geekdude" aria-label="Aaron Young">
          <img class="mm-team__img" alt="Aaron" src="https://gravatar.com/avatar/cdfc3774451a8ee9013e93a1d3d77143bad55d52056eb961339962ba565b2cbc?s=302" />
        </a>

        <h5 class="mm-team__name">Aaron Young</h5><p class="mm-team__role mm-team__role--empty"></p></div><div class="mm-team__member">
        <a class="mm-team__link" href="https://github.com/pedrovalerolara" aria-label="Pedro Valero-Lara">
          <img class="mm-team__img" alt="Pedro" src="https://avatars.githubusercontent.com/pedrovalerolara" />
        </a>

        <h5 class="mm-team__name">Pedro Valero-Lara</h5><p class="mm-team__role mm-team__role--empty"></p></div><div class="mm-team__member">
        <a class="mm-team__link" href="https://github.com/monil01" aria-label="Mohammad Alaul Haque Monil">
          <img class="mm-team__img" alt="Mohammad" src="https://avatars.githubusercontent.com/monil01" />
        </a>

        <h5 class="mm-team__name">Mohammad Alaul Haque Monil</h5><p class="mm-team__role mm-team__role--empty"></p></div><div class="mm-team__member">
        <a class="mm-team__link" href="https://github.com/zjin-lcf" aria-label="Zheming Jin">
          <img class="mm-team__img" alt="Zheming" src="https://avatars.githubusercontent.com/zjin-lcf" />
        </a>

        <h5 class="mm-team__name">Zheming Jin</h5><p class="mm-team__role mm-team__role--empty"></p></div><div class="mm-team__member">
        <a class="mm-team__link" href="https://github.com/spophale" aria-label="Swaroop Pophale">
          <img class="mm-team__img" alt="Swaroop" src="https://avatars.githubusercontent.com/spophale" />
        </a>

        <h5 class="mm-team__name">Swaroop Pophale</h5><p class="mm-team__role mm-team__role--empty"></p></div><div class="mm-team__member">
        <a class="mm-team__link" href="https://github.com/williamfgc" aria-label="William F. Godoy">
          <img class="mm-team__img" alt="William" src="https://avatars.githubusercontent.com/williamfgc" />
        </a>

        <h5 class="mm-team__name">William F. Godoy</h5><p class="mm-team__role mm-team__role--empty"></p></div><div class="mm-team__member">
        <a class="mm-team__link" href="https://github.com/keitaTN" aria-label="Keita Teranishi">
          <img class="mm-team__img" alt="Keita" src="https://avatars.githubusercontent.com/keitaTN" />
        </a>

        <h5 class="mm-team__name">Keita Teranishi</h5><p class="mm-team__role mm-team__role--empty"></p></div><div class="mm-team__member">
        <a class="mm-team__link" href="https://github.com/vetter" aria-label="Jeffrey S. Vetter">
          <img class="mm-team__img" alt="Jeffrey" src="https://avatars.githubusercontent.com/vetter" />
        </a>

        <h5 class="mm-team__name">Jeffrey S. Vetter</h5><p class="mm-team__role mm-team__role--empty"></p></div></div>
</section>



<h3 id=recent-posts class="archive__subtitle">Recent Posts</h3>






    </section>
  </article>
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    

    
      <li><a href="/ChatHPC/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2026 ORNL. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/ChatHPC/assets/js/main.min.js"></script>




<script src="/ChatHPC/assets/js/lunr/lunr.min.js"></script>
<script src="/ChatHPC/assets/js/lunr/lunr-store.js"></script>
<script src="/ChatHPC/assets/js/lunr/lunr-en.js"></script>





  
    <script src="/ChatHPC/assets/js/mm-collapse.js"></script>
  



  </body>
</html>
